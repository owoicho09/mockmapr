import asyncio
from playwright.async_api import async_playwright
import django
import os, sys, re, csv
from urllib.parse import urljoin, urlparse
from asgiref.sync import sync_to_async

import time
import sys

sys.stdout.reconfigure(encoding='utf-8')
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
APP_ROOT = os.path.abspath(os.path.join(CURRENT_DIR, "../../../.."))

sys.path.insert(0, APP_ROOT)
os.environ.setdefault("DJANGO_SETTINGS_MODULE", "core.settings")

import django
django.setup()

from mockmap.models import Lead   # <-- correct import for your app
from django.db import IntegrityError

# Enhanced email regex patterns
EMAIL_PATTERNS = [
    r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}',
    r'mailto:([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})',
    r'email[:\s]*([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})',
    r'contact[:\s]*([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})',
]

# Common pages to check for contact info
CONTACT_PAGES = [
    '/contact',
    '/contact-us',
    '/about',
    '/about-us',
    '/team',
    '/staff',
    '/get-in-touch',
    '/reach-out',
    '/connect',
    '/book',
    '/booking',
    '/consultation',
]


async def extract_emails_from_content(content):
    """Extract emails using multiple regex patterns"""
    print("    ðŸ” Starting email extraction from content...")
    emails = set()

    for i, pattern in enumerate(EMAIL_PATTERNS):
        print(f"    ðŸ“§ Trying pattern {i + 1}: {pattern}")
        matches = re.findall(pattern, content, re.IGNORECASE)
        print(f"    ðŸ“§ Pattern {i + 1} found {len(matches)} matches")

        if matches and isinstance(matches[0], tuple):
            pattern_emails = [match[0] for match in matches]
        else:
            pattern_emails = matches

        emails.update(pattern_emails)
        print(f"    ðŸ“§ Pattern {i + 1} emails: {pattern_emails}")

    print(f"    ðŸ“§ Total raw emails found: {len(emails)}")
    print(f"    ðŸ“§ Raw emails: {list(emails)}")

    # Enhanced filtering - exclude common non-personal emails and error tracking
    filtered_emails = []
    exclude_patterns = [
        r'noreply@', r'no-reply@', r'support@', r'hello@',
        r'admin@', r'webmaster@', r'postmaster@', r'mail@', r'contact@',
        r'example\.com', r'test\.com', r'placeholder', r'localhost',
        r'@facebook\.com', r'@twitter\.com', r'@instagram\.com',
        r'@linkedin\.com', r'@youtube\.com', r'@gmail\.com',
        r'@sentry\.', r'@sentry\.io', r'@sentry\.wixpress\.com',  # Sentry error tracking
        r'@wixpress\.com', r'@wix\.com',  # Wix platform emails
        r'@hubspot\.com', r'@mailchimp\.com', r'@constantcontact\.com',
        r'@sendgrid\.', r'@mailgun\.', r'@amazonaws\.com',
        r'[0-9a-f]{8}[0-9a-f]{4}[0-9a-f]{4}[0-9a-f]{4}[0-9a-f]{12}@',  # UUID-like patterns
        r'^[0-9a-f]{32}@',  # Hash-like patterns
    ]

    for email in emails:
        email_lower = email.lower()
        print(f"    ðŸ” Checking email: {email_lower}")

        excluded = False
        for pattern in exclude_patterns:
            if re.search(pattern, email_lower, re.IGNORECASE):
                print(f"    âŒ Excluded {email_lower} (matches pattern: {pattern})")
                excluded = True
                break

        if not excluded:
            print(f"    âœ… Accepted email: {email_lower}")
            filtered_emails.append(email_lower)

    print(f"    ðŸ“§ Final filtered emails: {len(filtered_emails)}")
    print(f"    ðŸ“§ Filtered emails list: {filtered_emails}")

    return list(set(filtered_emails))


async def extract_business_description(page):
    """Extract meaningful business description from various sources"""
    print("    ðŸ“ Starting business description extraction...")
    descriptions = []

    # Try different selectors for business descriptions
    selectors = [
        'meta[name="description"]',
        'meta[property="og:description"]',
        '[class*="about"]',
        '[class*="description"]',
        '[class*="intro"]',
        '[class*="mission"]',
        '[class*="vision"]',
        '[class*="services"]',
        'h1 + p',
        'h2 + p',
        '.hero p',
        '.banner p',
        'main p:first-of-type',
    ]

    for selector in selectors:
        try:
            print(f"    ðŸ“ Trying selector: {selector}")
            elements = await page.locator(selector).all()
            print(f"    ðŸ“ Found {len(elements)} elements for selector: {selector}")

            for element in elements:
                text = await element.text_content()
                if text and len(text.strip()) > 50:  # Only meaningful text
                    descriptions.append(text.strip())
                    print(f"    ðŸ“ Found description: {text.strip()[:100]}...")
        except Exception as e:
            print(f"    âš ï¸ Error with selector {selector}: {e}")
            continue

    # If no specific descriptions found, get general page text
    if not descriptions:
        print("    ðŸ“ No specific descriptions found, trying general paragraphs...")
        try:
            paragraphs = await page.locator("p").all_text_contents()
            print(f"    ðŸ“ Found {len(paragraphs)} paragraphs")
            descriptions = [p.strip() for p in paragraphs if len(p.strip()) > 50]
            print(f"    ðŸ“ Filtered to {len(descriptions)} meaningful paragraphs")
        except Exception as e:
            print(f"    âš ï¸ Error getting paragraphs: {e}")

    # Return best description (longest meaningful one)
    if descriptions:
        best_desc = max(descriptions, key=len)
        final_desc = best_desc[:500] if len(best_desc) > 500 else best_desc
        print(f"    ðŸ“ Selected best description: {final_desc[:100]}...")
        return final_desc

    print("    ðŸ“ No description found")
    return None


async def scrape_page_thoroughly(page, base_url):
    """Thoroughly scrape a page and related contact pages"""
    print(f"  ðŸ” Starting thorough scrape of: {base_url}")
    all_emails = set()
    description = None

    try:
        # First, extract from current page
        print("  ðŸ“„ Extracting from main page...")
        content = await page.content()
        print(f"  ðŸ“„ Page content length: {len(content)} characters")

        emails = await extract_emails_from_content(content)
        all_emails.update(emails)
        print(f"  ðŸ“„ Main page emails: {emails}")

        # Get description from current page
        if not description:
            description = await extract_business_description(page)

        # Check for contact links and visit them
        contact_links = []

        # Look for contact page links
        print("  ðŸ”— Building contact page URLs...")
        for contact_path in CONTACT_PAGES:
            try:
                contact_url = urljoin(base_url, contact_path)
                contact_links.append(contact_url)
                print(f"  ðŸ”— Added contact URL: {contact_url}")
            except Exception as e:
                print(f"  âš ï¸ Error building contact URL {contact_path}: {e}")
                continue

        # Also look for contact links in the page
        print("  ðŸ”— Looking for contact links in page...")
        try:
            links = await page.locator('a[href*="contact"], a[href*="about"]').all()
            print(f"  ðŸ”— Found {len(links)} contact/about links")

            for i, link in enumerate(links[:5]):  # Limit to avoid too many requests
                href = await link.get_attribute('href')
                if href:
                    full_url = urljoin(base_url, href)
                    contact_links.append(full_url)
                    print(f"  ðŸ”— Added link {i + 1}: {full_url}")
        except Exception as e:
            print(f"  âš ï¸ Error finding contact links: {e}")

        # Visit contact pages
        unique_contact_links = list(set(contact_links))[:3]  # Limit to 3 contact pages
        print(f"  ðŸ”— Will visit {len(unique_contact_links)} contact pages")

        for i, contact_url in enumerate(unique_contact_links):
            try:
                print(f"  ðŸ“§ [{i + 1}/{len(unique_contact_links)}] Checking contact page: {contact_url}")
                await page.goto(contact_url, timeout=15000)
                await page.wait_for_timeout(2000)

                contact_content = await page.content()
                print(f"  ðŸ“§ Contact page content length: {len(contact_content)} characters")

                contact_emails = await extract_emails_from_content(contact_content)
                all_emails.update(contact_emails)
                print(f"  ðŸ“§ Contact page emails: {contact_emails}")

                # Get description from contact page if not found yet
                if not description:
                    description = await extract_business_description(page)

            except Exception as e:
                print(f"  âš ï¸ Could not access contact page {contact_url}: {e}")
                continue

        final_emails = list(all_emails)
        print(f"  âœ… Total emails found: {len(final_emails)}")
        print(f"  âœ… Final email list: {final_emails}")
        print(f"  âœ… Description found: {'Yes' if description else 'No'}")

        return final_emails, description

    except Exception as e:
        print(f"  âŒ Error during thorough scraping: {e}")
        return [], None


# Create async versions of Django ORM operations
@sync_to_async
def get_leads_without_email():
    """Get all leads that don't have an email"""
    return list(Lead.objects.filter(email__isnull=True))


@sync_to_async
def update_lead_with_email(lead_id, email, description):
    """Update lead with scraped email and description"""
    lead = Lead.objects.get(lead_id=lead_id)
    lead.email = email
    if description:
        lead.note = description
    lead.save()
    return lead


async def process_database_and_scrape():
    """Main function to process database leads and scrape websites"""
    print(f"ðŸš€ Starting database processing and scraping...")

    async with async_playwright() as p:
        print("ðŸŒ Launching browser...")
        browser = await p.chromium.launch(
            headless=True,
            args=['--no-sandbox', '--disable-dev-shm-usage']
        )

        context = await browser.new_context(
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        )

        processed_count = 0
        successful_extractions = 0

        try:
            print("ðŸ“‚ Querying database for leads without email...")
            leads = await get_leads_without_email()
            print(f"ðŸ“Š Found {len(leads)} leads to process")

            for lead in leads:
                name = lead.name if hasattr(lead, 'name') else " "
                url = lead.website if hasattr(lead, 'website') else " "
                phone = lead.phone if hasattr(lead, 'phone') else None
                address = lead.address if hasattr(lead, 'address') else None

                processed_count += 1

                print(f"\n{'=' * 80}")
                print(f"ðŸ”— [{processed_count}/{len(leads)}] Processing: {name}")
                print(f"ðŸŒ URL: {url}")

                if not url:
                    print(f"âš ï¸ No URL found for {name}")
                    continue

                # Ensure URL has protocol
                if not url.startswith(('http://', 'https://')):
                    url = 'https://' + url
                    print(f"ðŸ”§ Fixed URL: {url}")

                try:
                    print(f"ðŸš€ Creating new page...")
                    page = await context.new_page()

                    print(f"ðŸŒ Navigating to: {url}")
                    await page.goto(url, timeout=30000, wait_until='domcontentloaded')
                    print(f"â±ï¸ Waiting for page to load...")
                    await page.wait_for_timeout(3000)

                    # Thorough scraping
                    print(f"ðŸ” Starting thorough scraping...")
                    emails, description = await scrape_page_thoroughly(page, url)

                    if emails:
                        print(f"âœ… Found {len(emails)} email(s): {', '.join(emails)}")

                        # Update the lead with the first email found
                        primary_email = emails[0]
                        print(f"  ðŸ’¾ Updating lead with email: {primary_email}")

                        await update_lead_with_email(lead.lead_id, primary_email, description)
                        print(f"  âœ… Updated lead: {primary_email}")

                        successful_extractions += 1
                    else:
                        print(f"âŒ No emails found for {name}")

                    print(f"ðŸ—‘ï¸ Closing page...")
                    await page.close()

                    # Add delay between requests to be respectful
                    print(f"â±ï¸ Waiting 2 seconds before next request...")
                    await asyncio.sleep(2)

                except Exception as e:
                    print(f"âŒ Failed to scrape {url}: {e}")
                    print(f"âŒ Error type: {type(e).__name__}")
                    continue

        except Exception as e:
            print(f"âŒ Error processing database: {e}")
        finally:
            print(f"ðŸš« Closing browser...")
            await browser.close()
            print(f"\nðŸ“Š FINAL SUMMARY:")
            print(f"ðŸ“Š Processed: {processed_count} websites")
            print(f"ðŸ“Š Successful extractions: {successful_extractions}")
            print(
                f"ðŸ“Š Success rate: {(successful_extractions / processed_count) * 100:.1f}%" if processed_count > 0 else "0%")
            subject = "Genesis Google Map Extraction Completed "

            # Calculate success rate with 1 decimal place
            if processed_count > 0:
                success_rate = f"{(successful_extractions / processed_count) * 100:.1f}%"
            else:
                success_rate = "0%"

            message = f"""
Hi Michael,

Here's your scraping update from Genesis.ai:

- ðŸ“Š Processed: {processed_count} websites
- ðŸ“Š Successful extractions: {successful_extractions}
-   Success rate: {success_rate}
Keep grinding ðŸ’ª

â€“ Genesis.ai Bot
            """


def run_email_extractor(verbose=True):
    """
    Run the email extractor on database leads.
    :param niche: Niche label to associate with scraped leads.
    :param verbose: Whether to print logs.
    """
    if verbose:
        print("ðŸŽ¯ EMAIL SCRAPER STARTING")
        print("=" * 80)
        print("\nðŸš€ Starting scraping process...")

    return asyncio.run(process_database_and_scrape())


import argparse

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Run email extractor on database leads.")


    run_email_extractor()